{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scalable Machine Learning in Python \n",
    "===================\n",
    "with Scikit-Learn and Dask \n",
    "===============\n",
    "## 4 - Dask and Scikit Learn\n",
    "**May 2017**\n",
    "\n",
    "<a href=http://dask.pydata.org ><img src=https://www.continuum.io/sites/default/files/dask_stacked.png\n",
    " width=200 />\n",
    "</a>\n",
    "\n",
    "[http://bit.ly/scaleml-dask-wkshp](http://bit.ly/scaleml-dask-wkshp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Validated Parameter Search\n",
    "------------------------------------\n",
    "\n",
    "In this section we present an open ended problem, cross-validated parameter search, and encourage students to try one of the previously mentioned techniques to parallelize it.  Any of `map`, `submit`, or collections like `spark` or `dask.bag` will work fine.\n",
    "\n",
    "\n",
    "### Requirements\n",
    "\n",
    "*  SciKit Learn\n",
    "*  A parallel computing framework of your choice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application\n",
    "\n",
    "We use grid search to find the optimal parameters for tuning a machine learning model.  This is slightly more complex than a map so we use `submit`.  We train the support vector classifier on handwritten digits using cross validation to avoid over-fitting.\n",
    "\n",
    "As before we start with a sequential solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()  # Collect Data\n",
    "\n",
    "plt.imshow(digits.data[75].reshape(8, 8),  # Example element\n",
    "           interpolation='nearest', cmap='Greys');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use three utility functions that we provide in the `cv_params_demo.py` module. The `load_cv_split` function splits the data into a training and test set. `evaluate_one` fits the model and scores it over the data for a particular set of tuning parameters. `plot_results` visualizes the model score over the sampled parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cv_params_demo import load_cv_split, evaluate_one, plot_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "`C`, `gamma`, and `tol` are all tunable parameters to the support-vector classifier, representing the penalty parameter of the error term, the kernel coefficient, and the stopping tolerance, respectively. Although `scikit-learn` can pick reasonable defaults for each of these, they can frequently be improved with additional knowledge of the data or by what we're doing here, randomly sampling the parameter space. We start with ten parameter samples, but can increase this after we've built our parallel solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CV_SPLIT_COUNT     = 3   # increase to 5 when parallel code is working\n",
    "PARAM_SAMPLE_COUNT = 10  # increase to 40 when parallel code is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "param_grid = {\n",
    "    'C': np.logspace(-10, 10, 1001),\n",
    "    'gamma': np.logspace(-10, 10, 1001),\n",
    "    'tol': np.logspace(-4, -1, 4),\n",
    "}\n",
    "\n",
    "param_samples = ParameterSampler(param_grid, PARAM_SAMPLE_COUNT)\n",
    "\n",
    "print(len(param_samples))\n",
    "list(param_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data for cross-validation\n",
    "\n",
    "For now, we'll only build two randomly-chosen splits of the data for training and testing. We can increase this number after we've built our parallel solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cv_params_demo import load_cv_split\n",
    "\n",
    "cv_splits = [load_cv_split(i) for i in range(CV_SPLIT_COUNT)]\n",
    "idx, (x_train, x_test, y_train, y_test) = cv_splits[0]\n",
    "x_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential cross validated parameter search\n",
    "\n",
    "The below code sequentially loops over the randomly created data splits and parameter samples to create a list of scored samples over the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "results = []\n",
    "\n",
    "for split in cv_splits:\n",
    "    for params in param_samples:\n",
    "        result = evaluate_one(SVC, params, split)\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results\n",
    "\n",
    "Which region of the parameter space is scoring well (higher is better)?  Are the number of samples we've computed sufficient to completely tune the model?  \n",
    "\n",
    "Searching over more parameters would help to improve the intuition we can gain here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cv_params_demo import plot_results\n",
    "\n",
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1 Parallel cross validated parameter search\n",
    "\n",
    "Try using some of the techniques we've used before (or other techniques altogether) to parallelize the above computation.  \n",
    "\n",
    "Afterwards, increase the number of parameters to help improve our understanding of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from distributed import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CV_SPLIT_COUNT     = 5   # increase to 5 when parallel code is working\n",
    "PARAM_SAMPLE_COUNT = 40  # increase to 40 when parallel code is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "param_grid = {\n",
    "    'C': np.logspace(-10, 10, 1001),\n",
    "    'gamma': np.logspace(-10, 10, 1001),\n",
    "    'tol': np.logspace(-4, -1, 4),\n",
    "}\n",
    "\n",
    "param_samples = ParameterSampler(param_grid, PARAM_SAMPLE_COUNT)\n",
    "\n",
    "print(len(param_samples))\n",
    "list(param_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_splits     = [load_cv_split(i) for i in range(CV_SPLIT_COUNT)]  # Increase the number 2 after parallel computation acheived\n",
    "param_samples = ParameterSampler(param_grid, PARAM_SAMPLE_COUNT)    # Increase the number 10 after parallel computation acheived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from dask import delayed, compute\n",
    "results = []\n",
    "\n",
    "for split in cv_splits:\n",
    "    for params in param_samples:\n",
    "        result = delayed(evaluate_one)(SVC, params, split)\n",
    "        results.append(result)\n",
    "        \n",
    "final = compute(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(final[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read More\n",
    "\n",
    "* [Dask SVD](http://matthewrocklin.com/blog/work/2015/06/26/Complex-Graphs)\n",
    "* [Blog post on Grid Search with Dask](http://matthewrocklin.com/blog/work/2017/02/07/dask-sklearn-simple)\n",
    "    * NOTE: The `dklearn` package has been reorganized and renamed\n",
    "    * This piece is now [dask-searchcv](https://github.com/dask/dask-searchcv)\n",
    "* [XGBoost and Dask Notebook](./X2 Dask XGBoost Example.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
