{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Scalable Machine Learning in Python \n",
    "===================\n",
    "with Scikit-Learn and Dask \n",
    "===============\n",
    "## 1 - Dask Task Graphs\n",
    "**May 2017**\n",
    "\n",
    "<a href=https://dask.pydata.org ><img src=https://www.continuum.io/sites/default/files/dask_stacked.png\n",
    " width=200 />\n",
    "</a>\n",
    "\n",
    "[http://bit.ly/scaleml-dask-wkshp](http://bit.ly/scaleml-dask-wkshp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### We have a strong analytics ecosystem (NumPy, Pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### that is mostly restricted to a single core and RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How do we parallelize an ecosystem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "of thousands of packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "each with custom algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sckit-Image: general image analysis\n",
    "\n",
    "    skimage.feature.canny(im, sigma=3)\n",
    "\n",
    "<img src=\"http://scikit-image.org/docs/dev/_images/sphx_glr_plot_canny_001.png\"\n",
    "     alt=\"Canny edge detection from skimage\"\n",
    "     width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Scikit-Allel: Specialized genomics\n",
    "\n",
    "<img src=\"http://alimanfoo.github.io/assets/2016-06-10-scikit-allel-tour_files/2016-06-10-scikit-allel-tour_50_0.png\" alt=\"scikit-allel example\" width=\"50%\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Need a parallel computing library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "... that is flexible enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "... and familiar enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "... to parallelize a disparate ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Outline\n",
    "-------\n",
    "\n",
    "-  Parallel NumPy and Pandas\n",
    "-  Parallel code generally\n",
    "-  Task Graphs and Task Scheduling\n",
    "    -   Compare with other systems (Spark, Airflow)\n",
    "    -   Dask's task schedulers\n",
    "-  Python APIs and Protocols\n",
    "-  Python Ecosystem and strengths for parallel computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Distributed Numpy  `dask.array`\n",
    "\n",
    "<img src=\"images/dask-array-black-text.svg\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# NumPy code\n",
    "import numpy as np\n",
    "x = np.random.random((1000, 1000))\n",
    "u, s, v = np.linalg.svd(x.dot(x.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Dask.array code\n",
    "import dask.array as da\n",
    "x = da.random.random((100000, 100000), chunks=(1000, 1000))\n",
    "u, s, v = da.linalg.svd(x.dot(x.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## `dask.dataframe`\n",
    "\n",
    "<img src=\"images/dask-dataframe.svg\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('myfile.csv', parse_dates=['timestamp'])\n",
    "df.groupby(df.timestamp.dt.hour).value.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "df = dd.read_csv('hdfs://myfiles.*.csv', parse_dates=['timestamp'])\n",
    "df.groupby(df.timestamp.dt.hour).value.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# But many problems aren't just big arrays and dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The Python community writes clever algorithms\n",
    "\n",
    "Fine Grained Python Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for a in A:\n",
    "    for b in B:\n",
    "        if a < b:\n",
    "            results[a, b] = f(a, b)\n",
    "        else:\n",
    "            results[a, b] = g(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Parallelizable, but not a list, dataframe, or array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from dask import delayed, compute\n",
    "\n",
    "results = {}\n",
    "\n",
    "for a in A:\n",
    "    for b in B:\n",
    "        if a < b:\n",
    "            results[a, b] = delayed(f)(a, b)  # lazily construct graph\n",
    "        else:\n",
    "            results[a, b] = delayed(g)(a, b)  # without structure\n",
    "\n",
    "results = compute(results)  # trigger all computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## `concurrent.futures.ThreadPoolExecutor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor \n",
    "\n",
    "e = ThreadPoolExecutor()\n",
    "\n",
    "results = {}\n",
    "\n",
    "for a in A:\n",
    "    for b in B:\n",
    "        if a < b:\n",
    "            results[a, b] = e.submit(f, a, b)  # submit work asynchronously\n",
    "        else:\n",
    "            results[a, b] = e.submit(g, a, b)  # submit work asynchronously\n",
    "\n",
    "results = {k: v.result() for k, v in results.items()} # block until finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dask APIs Produce Task Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "---\n",
    "# Dask Schedulers Execute Task Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy      as np\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1D-Array\n",
    "\n",
    "<img src=\"images/array-1d.svg\">\n",
    "\n",
    "    >>> np.ones((15,))\n",
    "    array([ 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
    "\n",
    "    >>> x = da.ones((15,), chunks=(5,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1D-Array\n",
    "\n",
    "<img src=\"images/array-1d-sum.svg\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = da.ones((15,), chunks=(5,))\n",
    "x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ND-Array - Sum\n",
    "\n",
    "<img src=\"images/array-sum.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = da.ones((15, 15), chunks=(5, 5))\n",
    "x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ND-Array - Transpose\n",
    "\n",
    "<img src=\"images/array-xxT.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = da.ones((15, 15), chunks=(5, 5))\n",
    "x + x.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ND-Array - Matrix Multiply\n",
    "\n",
    "<img src=\"images/array-xdotxT.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = da.ones((15, 15), chunks=(5, 5))\n",
    "x.dot(x.T + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ND-Array - Compound Operations\n",
    "\n",
    "<img src=\"images/array-xdotxT-mean.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = da.ones((15, 15), chunks=(5, 5))\n",
    "x.dot(x.T + 1) - x.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ND-Array - Compound Operations\n",
    "\n",
    "<img src=\"images/array-xdotxT-mean-std.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = da.ones((15, 15), chunks=(5, 5))\n",
    "y = (x.dot(x.T + 1) - x.mean()).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Dask APIs Produce Task Graphs\n",
    "\n",
    "<hr>\n",
    "\n",
    "# Dask Schedulers Execute Task Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise 1: Dask Arrays and Task Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ge = pd.read_csv('../data/minute/ge/2012-05-01.csv', parse_dates=['timestamp'])\n",
    "hp = pd.read_csv('../data/minute/hp/2012-05-01.csv', parse_dates=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge.plot(x='timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.plot(x='timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.close.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.close.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "hp_filenames = glob('../data/minute/hp/*.csv')\n",
    "len(hp_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hp_pd = pd.concat(map(pd.read_csv, hp_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_pd.plot(x='timestamp', title='HP', figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use `das.dataframe.read_csv()` to perform the same operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import dask.dataframe as dd\n",
    "hp_dd = dd.read_csv('../data/minute/hp/*.csv', parse_dates=['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** You will have to think about when to call the `.compute()` method.\n",
    "\n",
    "-  How many rows are in the `hp_dd` dataset?\n",
    "-  Get the *max* and *min* for the `high` column over the entire data set\n",
    "-  Get the *mean* for the `close` column over the entire data set\n",
    "-  Read the first few rows of the timestamp column\n",
    "-  Use the `.dt.round` method to round the timestamp column to days\n",
    "-  Get the high value for each day by grouping by the result from above and computing the maximum of the high column per group\n",
    "-  Compute the daily high-low spread.\n",
    "-  Plot the resulting Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = hp_dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(hp_dd.timestamp.dt.round('1d')).high.max().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hp_dd['high'].mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
